{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4747.74540501833, 27: 293.82346946001053, 16: 159.41741746664047, 43: 7.436751425266266, 53: 7.248833656311035, 45: 31.745578587055206, 60: 27.39535015821457, 42: 5.0009805560112, 44: 14.065178573131561, 58: 19.999642312526703, 41: 72.29670107364655, 26: 25.16674542427063, 25: 8.601253390312195, 34: 19.734644770622253, 18: 62.659197211265564, 67: 66.31527000665665, 56: 109.67237162590027, 5: 5.4802815318107605, 17: 18.092341005802155, 21: 21.076729476451874, 24: 24.661790311336517, 57: 29.105053544044495, 8: 13.188944399356842, 39: 52.230327010154724, 40: 12.590883135795593, 79: 11.952634453773499, 3: 9.803335130214691, 38: 2.6151154041290283, 2: 101.28770381212234, 28: 3.8886606693267822, 63: 33.4433159828186, 15: 32.78496390581131, 54: 5.734434962272644, 73: 38.39191633462906, 71: 7.049708664417267, 76: 6.135054588317871, 77: 12.465758919715881, 59: 33.958972811698914, 13: 18.2206472158432, 19: 12.149249970912933, 62: 9.163593411445618, 7: 27.408604562282562, 55: 3.8332515358924866, 66: 3.1392603516578674, 32: 18.1476548910141, 72: 3.0886791944503784, 48: 5.384914994239807, 74: 8.433342814445496, 14: 21.15441119670868, 69: 9.193825900554657, 61: 2.814272165298462, 68: 2.459808111190796, 78: 0.6326995491981506, 11: 1.457233726978302, 37: 2.809524714946747, 46: 2.363519012928009, 49: 1.9073315262794495, 47: 4.684715807437897, 4: 9.645717322826385, 33: 8.29574328660965, 64: 4.561085283756256, 65: 4.580968976020813, 35: 3.365145742893219, 1: 6.993192911148071, 20: 2.7317764163017273, 75: 7.1823864579200745, 9: 5.448369860649109, 29: 7.311134040355682, 6: 2.279798209667206, 50: 0.660298228263855, 52: 6.171478509902954, 31: 1.4510489702224731, 30: 1.1663846373558044, 22: 0.5370089411735535, 12: 0.5006898641586304, 23: 0.5516241192817688}\n",
      "{0: 8072.286660909653, 27: 669.2640487551689, 15: 114.00220662355423, 16: 301.3827106356621, 57: 55.96895241737366, 56: 211.36171585321426, 25: 17.944625735282898, 5: 10.328627049922943, 2: 243.25349479913712, 14: 69.51027178764343, 55: 37.07794940471649, 45: 116.08166128396988, 43: 16.676545917987823, 41: 185.5352668762207, 39: 129.65472894906998, 26: 49.709062576293945, 73: 44.75730311870575, 44: 37.97302597761154, 67: 99.91855102777481, 34: 23.36157923936844, 74: 27.463786780834198, 28: 13.405941307544708, 40: 46.9275296330452, 59: 74.56685018539429, 48: 14.830103278160095, 60: 64.9168319106102, 36: 6.779228687286377, 30: 3.4214531779289246, 65: 13.131971597671509, 18: 341.68093633651733, 63: 48.39214599132538, 7: 50.79945343732834, 24: 46.938541769981384, 19: 33.558408081531525, 58: 49.93206858634949, 71: 9.565180897712708, 37: 9.796539723873138, 17: 42.846943855285645, 33: 19.53808456659317, 21: 73.70296001434326, 46: 13.133975446224213, 32: 33.21774512529373, 1: 23.68598359823227, 77: 12.723986983299255, 50: 4.777625620365143, 75: 45.96670949459076, 62: 27.888488352298737, 79: 21.351843893527985, 69: 20.372378289699554, 38: 5.272127032279968, 42: 14.27969628572464, 3: 24.691308915615082, 53: 12.148032784461975, 13: 21.3041051030159, 68: 6.431330621242523, 8: 28.128334403038025, 4: 18.01658147573471, 54: 24.684099078178406, 66: 10.325835704803467, 49: 10.252713024616241, 76: 10.18807965517044, 61: 8.928702116012573, 12: 1.3151987791061401, 51: 8.72118878364563, 9: 13.574327886104584, 31: 3.3368701338768005, 29: 10.098488628864288, 78: 3.291885733604431, 35: 6.33655458688736, 64: 6.280970394611359, 72: 10.911978006362915, 52: 5.974595069885254, 10: 2.4530311226844788, 47: 7.197582960128784, 20: 8.017598390579224, 6: 8.631146848201752, 70: 0.6590467691421509, 22: 1.7892528772354126, 11: 1.3481196761131287}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dictionary={}\n",
    "hateful_or_not={}\n",
    "count=0\n",
    "with open (\"../data/train.jsonl\",'r') as f:\n",
    "    for line in f:\n",
    "        elem=json.loads(line)\n",
    "        if (int(elem['label'])==0):\n",
    "            hateful_or_not[int(elem['id'])]=0\n",
    "        else:\n",
    "            hateful_or_not[int(elem['id'])]=1\n",
    "            count+=1\n",
    "# print(count)\n",
    "probab_hateful=count/len(hateful_or_not)\n",
    "words_object_hateful={}\n",
    "words_object_normal={}\n",
    "with open(\"object_details.json\", 'r') as f:\n",
    "    dictionary=json.load(f)\n",
    "    for (image_id,elem) in dictionary.items():\n",
    "        # print(sentence)\n",
    "        for inndexing,id in enumerate(elem['class_ids']):\n",
    "            # if (id==11):\n",
    "                # print(image_id)\n",
    "            if (hateful_or_not[int(image_id)]==0):\n",
    "                if id in words_object_normal:\n",
    "                    words_object_normal[id]+=elem['confs'][inndexing]\n",
    "                else:\n",
    "                    words_object_normal[id]=elem['confs'][inndexing]\n",
    "            else:\n",
    "                if id in words_object_hateful:\n",
    "                    words_object_hateful[id]+=elem['confs'][inndexing]\n",
    "                else:\n",
    "                    words_object_hateful[id]=elem['confs'][inndexing]\n",
    "\n",
    "\n",
    "print(words_object_hateful)\n",
    "print(words_object_normal)\n",
    "with open(\"words_object_hateful.json\", 'w') as f:\n",
    "    json.dump(words_object_hateful,f)\n",
    "with open(\"words_object_normal.json\", 'w') as f:\n",
    "    json.dump(words_object_normal,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "probab_hateful=1/2\n",
    "import numpy as np\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_prior = {}\n",
    "        self.word_likelihoods = {}\n",
    "\n",
    "    def train(self, word_counts, hateful_dict, normal_dict):\n",
    "        # for val in hatef\n",
    "        total_hateful=0\n",
    "        total_normal=0\n",
    "        for val in hateful_dict.values():\n",
    "            total_hateful+=val\n",
    "        for val in normal_dict.values():\n",
    "            total_normal+=val\n",
    "        # total_hateful = sum(hateful_counts)\n",
    "        # total_normal = sum(normal_counts)\n",
    "        total_examples = total_hateful + total_normal\n",
    "        \n",
    "        # self.class_prior[1] = total_hateful / total_examples\n",
    "        # self.class_prior[0] = total_normal / total_examples\n",
    "        self.class_prior[1] = probab_hateful\n",
    "        self.class_prior[0] = 1 - probab_hateful\n",
    "        \n",
    "        for word in word_counts:\n",
    "            hateful_count=0\n",
    "            normal_count=0\n",
    "            if (word in hateful_dict):\n",
    "                hateful_count=int(hateful_dict[word])\n",
    "            if (word in normal_dict):\n",
    "                normal_count=int(normal_dict[word])\n",
    "            prob_hateful = (hateful_count + 1) / (total_hateful + len(word_counts))\n",
    "            prob_normal = (normal_count + 1) / (total_normal + len(word_counts))\n",
    "            self.word_likelihoods[word] = (prob_hateful, prob_normal)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for text in X_test:\n",
    "            prob_hateful = np.log(0.9996560693296879*self.class_prior[1])\n",
    "            prob_normal = np.log(self.class_prior[0])\n",
    "            for word, count in text.items():\n",
    "                if word in self.word_likelihoods:\n",
    "                    prob_hateful += count * np.log(self.word_likelihoods[word][0])\n",
    "                    prob_normal += count * np.log(self.word_likelihoods[word][1])\n",
    "                    # print(prob_hateful)\n",
    "                    # print(prob_normal)\n",
    "            predictions.append(1 if prob_hateful > prob_normal else 0)\n",
    "        return predictions\n",
    "\n",
    "# Example usage:\n",
    "# print(prob_hateful)\n",
    "# word_counts = [\"love\", \"hate\", \"awesome\", \"stupid\"]\n",
    "# hateful_counts = [1, 3, 0, 1]\n",
    "# normal_counts = [2, 0, 1, 2]\n",
    "word_counts=[]\n",
    "for word,count in words_object_hateful.items():\n",
    "    word_counts.append(word)\n",
    "for word,count in words_object_normal.items():\n",
    "    if word not in word_counts:\n",
    "        word_counts.append(word)\n",
    "\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(word_counts,words_object_hateful, words_object_normal)\n",
    "\n",
    "# X_test = [{11: 5}, {1: 1, 2: 1}]\n",
    "# predictions = classifier.predict(X_test)\n",
    "# print(predictions)  # Output: [1, 0] indicating hateful and non-hateful respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1]\n",
      "83 185\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GaussianNaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_prior = {}\n",
    "        self.word_stats = {}\n",
    "\n",
    "    def train(self, word_counts, hateful_dict, normal_dict):\n",
    "        total_hateful=0\n",
    "        total_normal=0\n",
    "        for val in hateful_dict.values():\n",
    "            total_hateful+=val\n",
    "        for val in normal_dict.values():\n",
    "            total_normal+=val\n",
    "        self.class_prior[1] = probab_hateful\n",
    "        self.class_prior[0] = 1 - probab_hateful\n",
    "        \n",
    "        # self.class_prior[1] = total_hateful / (total_hateful+total_normal)\n",
    "        # self.class_prior[0] = total_normal / (total_hateful+total_normal)\n",
    "        \n",
    "        for word in word_counts:\n",
    "            hateful_count=0\n",
    "            normal_count=0\n",
    "            if (word in hateful_dict):\n",
    "                hateful_count=int(hateful_dict[word])\n",
    "            if (word in normal_dict):\n",
    "                normal_count=int(normal_dict[word])\n",
    "            mean_hateful = hateful_count / total_hateful\n",
    "            mean_normal = normal_count / total_normal\n",
    "            variance_hateful = (hateful_count * (1 - mean_hateful)**2 + (total_hateful - hateful_count) * (0 - mean_hateful)**2) / total_hateful\n",
    "            variance_normal = (normal_count * (1 - mean_normal)**2 + (total_normal - normal_count) * (0 - mean_normal)**2) / total_normal\n",
    "            self.word_stats[word] = (mean_hateful, variance_hateful, mean_normal, variance_normal)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for text in X_test:\n",
    "            prob_hateful = np.log(self.class_prior[1])\n",
    "            prob_normal = np.log(self.class_prior[0])\n",
    "            for word, count in text.items():\n",
    "                if word in self.word_stats:\n",
    "                    mean_hateful, variance_hateful, mean_normal, variance_normal = self.word_stats[word]\n",
    "                    prob_hateful += self.gaussian_prob(count, mean_hateful, variance_hateful)\n",
    "                    prob_normal += self.gaussian_prob(count, mean_normal, variance_normal)\n",
    "            predictions.append(1 if prob_hateful > prob_normal else 0)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def gaussian_prob(self, x, mean, variance):\n",
    "        exponent = -((x - mean) ** 2) / (2 * (variance+1e-9))\n",
    "        return np.exp(exponent) / np.sqrt(2 * np.pi * (variance+1e-9))\n",
    "\n",
    "# Example usage:\n",
    "# word_counts = [\"love\", \"hate\", \"awesome\", \"stupid\"]\n",
    "# hateful_counts = [1, 3, 0, 1]\n",
    "# normal_counts = [2, 0, 1, 2]\n",
    "\n",
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(word_counts, words_object_hateful, words_object_normal)\n",
    "\n",
    "# X_test = [{11: 0}, { 1:0}]\n",
    "with open (\"object_details_test.json\") as f:\n",
    "    object_details_test=json.load(f)\n",
    "X_test=[]\n",
    "answers=[]\n",
    "for image_id,elem in object_details_test.items():\n",
    "    for index,id in enumerate(elem['class_ids']):\n",
    "        temp_dict={id:elem['confs'][index]}\n",
    "    X_test.append(temp_dict)\n",
    "    answers.append(elem['label'])\n",
    "\n",
    "# from decimal import Decimal\n",
    "\n",
    "# low = Decimal('0.7')\n",
    "# high = Decimal('1.5')\n",
    "# precision = Decimal('1e-6')\n",
    "\n",
    "# while high - low >= precision:\n",
    "#     mid1 = low + (high - low) / Decimal('3')\n",
    "#     mid2 = high - (high - low) / Decimal('3')\n",
    "    \n",
    "#     predictions1 = classifier.predict(X_test, float(mid1))\n",
    "#     predictions2 = classifier.predict(X_test, float(mid2))\n",
    "    \n",
    "#     count1 = sum(1 for answer, predict in zip(answers, predictions1) if answer == predict)\n",
    "#     count2 = sum(1 for answer, predict in zip(answers, predictions2) if answer == predict)\n",
    "    \n",
    "#     if count1 > count2:\n",
    "#         high = mid2\n",
    "#     else:\n",
    "#         low = mid1\n",
    "\n",
    "# best_factor = (low + high) / Decimal('2')\n",
    "\n",
    "# print(\"Best factor:\", float(best_factor))\n",
    "# print(\"Max count:\", max(count1, count2))\n",
    "\n",
    "# low = 0.7\n",
    "# high = 1.3\n",
    "# max_count = 0\n",
    "# best_factor = low\n",
    "\n",
    "# while high - low >= 1e-6:\n",
    "#     mid = (low + high) / 2\n",
    "#     predictions = classifier.predict(X_test, mid)\n",
    "#     count = 0\n",
    "#     count2 = 0\n",
    "#     for answer, predict in zip(answers, predictions):\n",
    "#         if answer == predict:\n",
    "#             if answer == 0:\n",
    "#                 count += 1\n",
    "#             else:\n",
    "#                 count2 += 1\n",
    "#     total_count = count + count2\n",
    "#     if total_count > max_count:\n",
    "#         max_count = total_count\n",
    "#         best_factor = mid\n",
    "#     if count > count2:\n",
    "#         low = mid\n",
    "#     else:\n",
    "#         high = mid\n",
    "\n",
    "# print(\"Best factor:\", best_factor)\n",
    "# print(\"Max count:\", max_count)\n",
    "\n",
    "\n",
    "predictions = classifier.predict(X_test)\n",
    "print(predictions)  # Output: [1, 0] indicating hateful and non-hateful respectively\n",
    "count=0\n",
    "count2=0\n",
    "for answer,predict in zip(answers,predictions):\n",
    "    if (answer==predict):\n",
    "        if (answer==0):\n",
    "            count+=1\n",
    "        else:\n",
    "            count2+=1\n",
    "        # print(\"Haash\")\n",
    "        # count+=1\n",
    "print(count,count2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
